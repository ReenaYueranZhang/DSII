---
title: "P8106 Data ScienceII Homework3"
author: "Yueran Zhang (yz4188)"
date: '2023-03-21'
output:
  pdf_document:
    toc_depth: 2
  html_document:
    toc_depth: '2'
---

In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the dataset “auto.csv”. The dataset contains 392 observations. The response variable is mpg cat, which indicates whether the miles per gallon of a car is high or low.

Split the dataset into two parts: training data (70%) and test data (30%).

# R Package
```{r R package, echo = T, message = FALSE, results='hide', warning=FALSE}
library(tidyverse)
library(knitr)
library(AppliedPredictiveModeling)
library(pROC)
library(caret)
library(klaR)
library(MASS)
```

# Import Dataset

```{r Dataset Import,echo = T, message = FALSE, results='hide', warning=FALSE}

set.seed(1234)

# Load dataset + clean data
auto = read.csv("/Users/yueranzhang/Desktop/DSII/HW3/DataSet/auto.csv") %>% 
janitor::clean_names() %>% 
  na.omit() %>% 
  distinct() %>% 
  mutate(
    cylinders = as.factor(cylinders),
    year = as.factor(year),
    origin = case_when(origin == "1" ~ "American",
                       origin == "2" ~ "European",
                       origin == "3" ~ "Japanese"),
    origin = as.factor(origin),
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, "low")
  ) %>% 
  as.data.frame()

# Data Partition
rowTrain <- createDataPartition(y = auto$mpg_cat,
                                p = 0.7,
                                list = FALSE) 

# training data
x <- model.matrix(mpg_cat~.,auto)[rowTrain]
y <- auto$mpg_cat[rowTrain]
# test data
x2 <- model.matrix(mpg_cat~.,auto)[-rowTrain]
y2 <- auto$mpg_cat[-rowTrain]

```


# Question A
### Perform a logistic regression using the training data. Do any of the predictors appear to be statistically significant? If so, which ones? Set a probability threshold to determine class labels and compute the confusion matrix using the test data. Briefly explain what the confusion matrix is telling you.

```{r QuestionA-glm}
set.seed(1234)
contrasts(auto$mpg_cat)
glm.fit <- glm(mpg_cat ~ .,
               data = auto,
               subset = rowTrain,
               family = binomial(link = "logit"))

# Check for statistically significant predictors
summary(glm.fit)

```

* After performing a glm model using the training data, the variables `weight(Vehicle weight)`, `year79(Model year 79)`, `year80`, `year81`, `year82` and `origin2(European origin)` appear to be statistically significant.

```{r QuestionA-Classification}

# By default, we set this classification threshold to 0.5. 


test.pred.prob = predict(glm.fit, newdata = auto[-rowTrain,],
                           type = "response")
test.pred = rep("low", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] = "high"

confusionMatrix(data = fct_rev(as.factor(test.pred)), # Reverse order of factor levels:)
                reference = auto$mpg_cat[-rowTrain],
                 positive = "high")


## Double check for the order 
# fct_rev(levels(as.factor(test.pred)))
# levels(auto$mpg_cat[-rowTrain])

```

```{r QuestionA-ROC plot}

set.seed(1234)
roc.glm <- roc(auto$mpg_cat[-rowTrain], test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```

* In the confusion matrix, the class of interest/our target class is high gas mileage (positive) class.
*  The diagonal elements are the correctly predicted samples. A total of `52+53=105` samples were correctly predicted out of the total `52+5+6+53=116` samples. The accuracy score reads as `0.9052(90.52%)` 95%CI  `(0.8058, 0.9324)` for the given data and observations. If a model will perform at 90.52% accuracy then the error rate will be `1-0.905 =9.5%`. Other important indicators, like specificity(0.8966) and sensitivity(0.9138).
* The confusion matrix also tells us that our no information rate is 50%, which means that if we had no information and made the same class prediction for all observations, our model would be 50% accurate.
* Our p-value is <2e-16 (close to 0) tells us that our accuracy is statistically significantly better than our no information rate. 
* The model' sensitivity is 91.38% (true detected positives out of all actual positives);Specificity is 84.48% (true detected negatives out of all actual negatives);positive predictive value is 85.48% (true detected positives out of all predicted positives); negative predictive value of 90.74% (true detected negatives out of all predicted negatives). Our sensitivity and specificity average is 87.93%, which is our balanced accuracy. Our kappa is 0.7586, indicates pretty good reliability.
* Another important metric that measures the overall performance of a classifier is the “Area Under ROC” (AUC) value.We also made this plot and observe the area measured under the ROC curve. A higher value of AUC represents a better classifier. The AUC of the practical learner above is 90% (from the plot we know AUC=0.957) which is a pretty good score.

# Question B
### Train a multivariate adaptive regression spline (MARS) model using the training data.

```{r QuestionB-MARS, warning=FALSE}

set.seed(1234)

ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

model.mars <- train(x = auto[rowTrain,1:7],
                    y = auto$mpg_cat[rowTrain],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3, 
                                           nprune = 3:25),
                    metric = "ROC",
                    trControl = ctrl)

summary(model.mars)

plot(model.mars)

model.mars$bestTune

coef(model.mars$finalModel) 


```

* For MARS model, Earth selected 12 of 26 terms, and 5 of 22 predictors (nprune=12). R-Squared(RSq) is `0.8283498`, indicates the test set with relatively higher variation in the reponse. 

# Question C
### Perform LDA using the training data. Plot the linear discriminants in LDA.
```{r QuestionC-LDA,warning=FALSE}

set.seed(1234)
par(mar=c(1,1,1,1))
lda.fit <- lda(mpg_cat~., data = auto,
               subset = rowTrain)

par(mar = rep(2,4))

plot(lda.fit)

lda.fit$scaling

```

* There are two classes in the LDA model, so we can plot 1 `(k = 2-1 = 1)` linear discriminant.


```{r,error=FALSE, warning=FALSE}
# Alternatively, let's use caret for LDA

set.seed(1234)

traindf <- auto[rowTrain,]

ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

model.lda <- train(mpg_cat ~ .,
                   data = traindf,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
model.lda$results
```


# Question D
### Which model will you use to predict the response variable? Plot its ROC curve using the test data. Report the AUC and the misclassification error rate.

```{r QuestionD-Resampling, warning=FALSE}

set.seed(1234)
logit.caret = train(x = rowTrain,
                    y = y,
                    method = "glm",
                    metric = "ROC",
                    trControl = ctrl)

res = resamples(list(LOGISTIC = logit.caret,
                     MARS = model.mars,
                     LDA = model.lda))

summary(res)
bwplot(res, metric = "ROC")

```

* Based on resampling from how our models perform on the training data, I prefer to use the LDA model for classification of our response variable `mpg_cat`, since it has the highest ROC.

```{r QuestionD-LDA,warning=FALSE}
#  ROC and prediction 
lda.predict = predict(model.lda, newdata = auto[-rowTrain, 1:7], type = "prob")[,2]
roc.lda = roc(auto$mpg_cat[-rowTrain], lda.predict)

#  AUC and misclassification rate
auc_lda = roc.lda$auc[1]
auc_lda

# Obtain the classes
lda_class = lda.predict %>% 
  as.data.frame() %>% 
  mutate(
    class = case_when(. < 0.50 ~ "low",
                      . > 0.50 ~ "high")
  ) %>% 
  dplyr::select(class) %>% 
  as.matrix()

# Confusion matrix and misclassification error rate
confusionMatrix(data = fct_rev(as.factor(lda_class)),
                reference = auto$mpg_cat[-rowTrain],
                positive = "high")

# Plot ROC Curve
modelName = "LDA model"
pROC::ggroc(list(roc.lda), legacy.axes = TRUE) + 
  scale_color_discrete(labels = paste0(modelName, " (", round(auc_lda, 2),")"),
                       name = "Model Type (AUC)") + 
  geom_abline(intercept = 0, slope = 1, color = "gold")
```
 
 * The LDA model has a misclassification rate of `1 - 0.8534 = 14.7%` , and when we apply threshold of 0.5 probability, and observe the area measured under the ROC curve(AUC). A higher value of AUC represents a better classifier. The AUC of the practical learner above is 90% (from the plot we know AUC=0.94) which is a pretty good score.
