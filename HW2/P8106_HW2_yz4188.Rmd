---
title: "P8106 Data ScienceII Homework2"
author: "Yueran Zhang(yz4188)"
date: '2023-03-05'
output:
  pdf_document:
    toc_depth: 2
  html_document:
    toc_depth: '2'
---

In this exercise, we build nonlinear models using the “College” data. The dataset contains statistics for 565 US Colleges from a previous issue of US News and World Report. The response variable is the out-of-state tuition (Outstate).Partition the dataset into two parts: training data (80%) and test data (20%).

# R Package
```{r R package, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(tidyverse)
library(ggplot2)
library(gridExtra)
```

# Import Dataset

```{r Dataset Import,echo = T, message = FALSE, results='hide', warning=FALSE}

set.seed(123)

# Load dataset + clean data
College = read.csv("/Users/yueranzhang/Desktop/DSII/HW2/DataSet/College.csv")[-1] %>% 
janitor::clean_names() %>% 
na.omit()

# Data Partition
RowTrain <- createDataPartition(y = College$outstate,
                                p = 0.8,
                                list = FALSE)

train_data <- College[RowTrain,]
test_data <- College[-RowTrain,]


# matrix of predictors
x <- model.matrix(outstate ~. , train_data) [,-1]
# vector of response
y <- train_data$outstate  

```

\newpage

# Question A
Fit smoothing spline models using perc.alumni as the only predictor of Outstate for a range of degrees of freedom, as well as the degree of freedom obtained by generalized cross-validation, and plot the resulting fits. Describe the results obtained.

```{r QuestionA}

set.seed(123)

perc_alumni.grid <- seq(from =0, to = 70, by = 1)
fit.ss <- smooth.spline(train_data$perc_alumni, train_data$outstate,cv = TRUE)

fit.ss$df

fit.ss$lambda

# plot the fit

pred.ss <- predict(fit.ss,
                   x =perc_alumni.grid)

pred.ss.df <- data.frame(pred = pred.ss$y,
                         perc_alumni = perc_alumni.grid)

p <- ggplot(data = train_data, aes(x = perc_alumni, y = outstate)) +
     geom_point(color = rgb(.2, .4, .2, .5))

p +
geom_line(aes(x = perc_alumni.grid, y = pred), data = pred.ss.df,
          color = rgb(.8, .1, .1, 1)) + theme_bw()
```

* The degree of freedom that we obtained by generalized cross-validation is `r fit.ss$df`. From the above plot, the smoothing spline is nearly to a linear line, or that's to say this model fits the data quite well.
* It is noticed that as we get the $\lambda$ value is `r fit.ss$lambda`, which is pretty large, so for the function estimate  $f_\lambda$ is essentially constrained to have a zero penalty, and it is forced to be smoother.


# Question B
Fit a generalized additive model (GAM) using all the predictors. Does your GAM model include all the predictors? Plot the results and explain your findings. Report the test error.

```{r QuestionB-1}

set.seed(123)
                                
ctrl <- trainControl(method = "cv", number = 10)

gam.fit <- train(x, y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = TRUE),
                 trControl = ctrl)

gam.fit

gam.fit$finalModel

```

* From the above output, there are 16 predictors, which means the GAM model include all the predictors.

```{r QuestionB-2}

set.seed(123)

summary(gam.fit)

# Plot of each predictor versus the outcome variable (outstate)
plot(gam.fit$finalModel, pages = 6)

```

* According to p-value, for some predictors, there is not sufficient evidence in the data to conclude they are significant association with the outcome variable outstate at the 5% significance level, like **terminal(Pct. of faculty with terminal degree)**, **ph_d(ct. of faculty with Ph.D.’s)**,**top25perc(Pct. new students from top 25% of H.S. class)**, and **p_undergrad(Number of parttime undergraduates)**. However, some of the predictors seems to have linear relationship with the model, such as **grad_rate( Graduation rate)** and **personal(Estimated personal spending)**.

* The Deviance explained by the model is 83.7%, and the adjusted R-Square is 81.5%, that showing a high level of correlation. The GAM model fits the data pretty well.


```{r Question B-3}
set.seed(123)

# Test Error(MSE)
testdata_x = test_data %>% 
select(-outstate)

gam.pred <- predict(gam.fit, newdata = testdata_x)

mean((gam.pred - test_data$outstate)^2)

```

* The test error of the GAM model is `r mean((gam.pred - test_data$outstate)^2) `.


# Question C

Train a multivariate adaptive regression spline (MARS) model using all the predictors. Report the final model. Present the partial dependence plot of an arbitrary predictor in your final model. Report the test error.

```{r QuestionC-MARS model}
mars_grid <- expand.grid(degree = 1:3, 
                         nprune = 2:20)

set.seed(123)
mars.fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl)

# Plot of gird tuning
ggplot(mars.fit)

# final model
mars.fit$bestTune

# coefficient of the MARS model
coef(mars.fit$finalModel) 
```

* From the above output, the final model has 16 coefficients, with degree of freedom is 1.

* We observed that variables such as **grad_rate( Graduation rate)** , **f_undergrad(Number of fulltime undergraduates)** and **perc_alumni(Pct. alumni who donate)** with larger absolute value, which means these variable may more likely to change the mean in the response given a one unit change in these predictor.

```{r QuestionC- PDPs plots}


# Plot of expend variable
p1 <- pdp::partial(mars.fit, pred.var = c("expend"), grid.resolution = 10) %>% autoplot()

# Plot of expend and room board variable
p2 <- pdp::partial(mars.fit, pred.var = c("expend", "room_board"), 
                   grid.resolution = 10) %>%
      pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
                       screen = list(z = 20, x = -60))

grid.arrange(p1, p2, ncol = 2)
```

* Here are two PDPs (Partial Dependence Plot) plots, the left one is with the variable **expend**, and the right side is for **expend** and **rood_board**.

```{r QuestionC-test error}
set.seed(123)

# Test Error(MSE)
testdata_x = test_data %>% 
select(-outstate)

mars.pred <- predict(mars.fit, newdata = testdata_x)

mean((mars.pred  - test_data$outstate)^2)

```

* The test error for MARS model `r mean((mars.pred  - test_data$outstate)^2)`.


